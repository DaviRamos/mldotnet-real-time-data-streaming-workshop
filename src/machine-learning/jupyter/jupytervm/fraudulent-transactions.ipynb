{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "We'll use Pandas to read the data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get to know your data set\n",
    "We can start by exploring the various data types in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From our data, it is clear that our feature set most likely should consist of the column step, to newbalanceDest while the label column is isFraud\n",
    "Let's create two variables holding the X and Y values for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns[:9]]\n",
    "Y = df[df.columns[9:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Transform the data\n",
    "Tranforming the data includes multiple steps, e.g.:\n",
    "- Replacing any missing values, infinity or NaN\n",
    "- Transforming textual features\n",
    "- Normalize the feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore missing values\n",
    "Running the following command indicates that we do not have any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform textual features\n",
    "The type, nameOrig and nameDest are textual features which we need to transform to a shape the machine learning algorithm can understand.\n",
    "The type column has finitive options, so we can do something called OneHotEncoding on the column to create binary columns.\n",
    "\n",
    "Pandas provide a very useful function for this, get_dummies. We can utilize this function to create new binary columns, drop the original column and concatenate the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X, pd.get_dummies(X['type'], prefix='type')], axis=1)\n",
    "\n",
    "X.drop(['type'],axis=1, inplace=True)\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X, pd.get_dummies(X['nameOrig'], prefix='nameOrig', sparse=True)], axis=1)\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming nameOrig and nameDest\n",
    "The following two columns to transform are nameOrig and nameDest\n",
    "We can use a label encoder for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_feature_mask = X.dtypes==object\n",
    "categorical_cols = X.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "X[categorical_cols] = X[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data\n",
    "To evaluate the performance of the model, we will need a dataset for training and one for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_train.values.flatten()\n",
    "Y_test = Y_test.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0, class_weight='balanced')\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model\n",
    "Once the model has been trained, lets evaluate how accurate the model is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = clf.score(X_test, Y_test)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Not bad, 99.9% accurate! But remember that the dataset was highly unbalanced (we can just guess non-fraudulent on all transactions and get 99.9% accuracy)\n",
    "Let's look at other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision-Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Precision**: High precision means that all transactions we flagged as fraudulent actually where (no false positives)\n",
    "- **Recall**: High recall means that we did not miss any fraudulent transactions (no false negatives)\n",
    "\n",
    "There is a trade off between both, lets see how we can illustrate that in a precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['non-fraudulent', 'fraudulent']\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "print(classification_report(Y_test,pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! We can see that for fraudulent transactions, our recall is very high, but precision is low. This means that we create quite a few false positives but that is better than missing fraudulent activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Y_score = clf.predict_proba(X_test)\n",
    "Y_score = Y_score[:, 1]\n",
    "Y_score = Y_score.flatten()\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(Y_test, Y_score)\n",
    "\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\n",
    "plt.legend(loc=\"center left\")\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate\n",
    "So our model is far from great, let's see if we can improve it. \n",
    "One of the issues we are seeing right now is that despite it's accuracy, it is missing a lot of fraudulent transactions.\n",
    "A struggle we have is that our dataset is highly imbalanced. We can address this by weighing our inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model again, but boosting our minority class, is fraudulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_with_weights = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0, class_weight=\"balanced\")\n",
    "clf_with_weights.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = clf_with_weights.score(X_test, Y_test)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuarcy dropped, but let's see if we can outweight that with better recall-precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['non-fraudulent', 'fraudulent']\n",
    "\n",
    "pred_with_weights = clf_with_weights.predict(X_test)\n",
    "print(classification_report(Y_test, pred_with_weights, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! We can see that especially recall greatly increased for the fraudulent cases, meaning that we miss a lot less false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Y_score = clf_with_weights.predict_proba(X_test)\n",
    "Y_score = Y_score[:, 1]\n",
    "Y_score = Y_score.flatten()\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(Y_test, Y_score)\n",
    "\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\n",
    "plt.legend(loc=\"center left\")\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
